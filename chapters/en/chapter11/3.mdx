# Supervised Fine-Tuning

Supervised Fine-Tuning (SFT) is a critical process for adapting pre-trained language models to specific tasks or domains. While pre-trained models have impressive general capabilities, they often need to be customized to excel at particular use cases. SFT bridges this gap by further training the model on relevant datasets with human-validated examples.

Because of the supervised structure of the task, the model can learn to generate structured outputs. For example, the chat templates we created in the previous sections.

## Understanding Supervised Fine-Tuning

Supervised fine-tuning is about teaching a pre-trained model to perform specific tasks, and use specific output structures, through examples of labeled tokens. The process involves showing the model many examples of the desired input-output behavior, allowing it to learn the patterns specific to your use case. 

SFT is effective because it uses the foundational knowledge acquired during pre-training while adapting the model's behavior to match your specific needs.

## When to Use Supervised Fine-Tuning

The decision to use SFT often comes down to the gap between your model's current capabilities and your specific requirements. SFT becomes particularly valuable when you need precise control over the model's outputs or when working in specialized domains.

Two core reasons to use SFT are:

1. **Template Control**: SFT allows you to control the output structure of the model, ensuring that it generates outputs in a specific format. For example, you need a specific chat template to generate structured outputs.

2. **Domain-Specific Requirements**: SFT is effective when you need precise control over the model's outputs in specialized domains. For example, if you're developing a customer service application, you might want your model to consistently follow company guidelines and handle technical queries in a standardized way. SFT can help align the model's responses with professional standards and domain expertise.

## Quiz

### 1. What is the primary purpose of Supervised Fine-Tuning (SFT)?

<Question
	choices={[
		{
			text: "To train a language model from scratch",
			explain: "SFT builds upon pre-trained models rather than training from scratch."
		},
		{
			text: "To adapt a pre-trained model to specific tasks or domains while maintaining its foundational knowledge",
			explain: "Correct! SFT allows models to learn specific tasks while leveraging their pre-trained capabilities.",
			correct: true
		},
		{
			text: "To compress a large language model into a smaller one",
			explain: "This is more related to model distillation, not SFT."
		}
	]}
/>

### 2. Which of the following are valid reasons to use SFT?

<Question
	choices={[
		{
			text: "Template Control - ensuring the model generates outputs in a specific format",
			explain: "Yes! SFT helps enforce specific output structures through training examples.",
			correct: true
		},
		{
			text: "Domain Adaptation - teaching the model domain-specific knowledge and terminology",
			explain: "Correct! SFT is excellent for adapting models to specialized domains.",
			correct: true
		},
		{
			text: "Model Architecture Changes - modifying the underlying structure of the model",
			explain: "SFT doesn't change the model architecture, it only updates the weights."
		}
	]}
/>

### 3. What is required for effective Supervised Fine-Tuning?

<Question
	choices={[
		{
			text: "A pre-trained language model",
			explain: "Yes! SFT starts with a pre-trained model as its foundation.",
			correct: true
		},
		{
			text: "Validated examples of desired input-output behavior",
			explain: "Correct! Quality training data is crucial for successful SFT.",
			correct: true
		},
		{
			text: "A high performing reference model",
			explain: "SFT uses existing architectures rather than creating new ones."
		}
	]}
/>

### 4. How does SFT relate to chat templates?

<Question
	choices={[
		{
			text: "SFT can train models to consistently follow specific chat templates",
			explain: "Correct! SFT helps models learn to generate responses in the desired template format.",
			correct: true
		},
		{
			text: "Chat templates are not compatible with SFT",
			explain: "Incorrect! Chat templates are commonly used with SFT for structured outputs."
		},
		{
			text: "SFT automatically creates chat templates",
			explain: "SFT doesn't create templates, it trains models to use existing templates."
		}
	]}
/>

### 5. What distinguishes SFT from pre-training?

<Question
	choices={[
		{
			text: "SFT uses labeled data for specific tasks",
			explain: "Yes! SFT requires examples of desired behavior for specific tasks.",
			correct: true
		},
		{
			text: "SFT is faster than pre-training",
			explain: "The speed difference isn't a defining characteristic; it depends on various factors."
		},
		{
			text: "SFT requires more data than pre-training",
			explain: "Actually, SFT typically uses less data than pre-training, focusing on task-specific examples."
		}
	]}
/>