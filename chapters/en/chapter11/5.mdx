# Fine-Tuning Process with SFTTrainer in Transformers Reinforcement Learning

The supervised fine-tuning process involves adjusting a model's weights on a task-specific dataset. Let's work through the process step by step.

## Dataset Preparation

Your dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model. The data format needs to be compatible with the model's chat template. Below is an example of a dataset that can be used for supervised fine-tuning.

<iframe
  src="https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0"
  frameborder="0"
  width="100%"
  height="360px"
></iframe>

## Training Configuration

We will configure SFT trainer with the following parameters:

| Parameter | Description |
|-----------|-------------|
| num_train_epochs | The total number of training epochs to run (e.g., 1-3 epochs) |
| per_device_train_batch_size | The number of training examples processed per GPU in one forward/backward pass (typically 2-8 for large models) |
| gradient_accumulation_steps | Number of updates to accumulate before performing a backward pass, effectively increasing batch size |
| learning_rate | The step size for model weight updates during training (typically 2e-4 for fine-tuning) |
| gradient_checkpointing | Memory optimization technique that trades computation for memory by recomputing intermediate activations |
| warmup_ratio | Portion of training steps used for learning rate warmup (e.g., 0.03 = 3% of steps) |
| logging_steps | Frequency of logging training metrics and progress (e.g., every 10 steps) |
| save_strategy | When to save model checkpoints (e.g., "epoch" saves after each epoch, "steps" saves every N steps) |

In general, start with a small number of epochs and data using the default parameters in `trl.SFTTrainer`. As you get more comfortable with the process, you can experiment with different configurations to see how they affect the model's performance.

## Training and Evaluation

Fortunately, the `SFTTrainer` class handles the training and evaluation process for us. We just need to pass in the appropriate parameters and call the `train()` method. For the sake of education, let's break down what happens behind the scenes.

- Iterating over the dataset
- Computing the loss
- Updating the model's parameters
- Regular evaluation on a validation set

Throughout the process, continuous evaluation is essential. You'll want to monitor the model's performance on a validation set to ensure it's learning the desired behaviors without losing its general capabilities. 

## `SFTTrainer` from Transformer Reinforcement Learning

Transformer Reinforcement Learning (TRL) is a toolkit used to train transformer language models using reinforcement learning (RL) and post-training techniques. Built on top of the Hugging Face Transformers library, TRL allows users to directly load pretrained language models and supports most decoder and encoder-decoder architectures. The library facilitates major processes of RL used in language modelling, including supervised fine-tuning (SFT), reward modeling (RM), proximal policy optimization (PPO), and Direct Preference Optimization (DPO).

Here's a basic simplified example of how to use `SFTTrainer` to fine-tune a model.

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset(path="HuggingFaceTB/smoltalk", name="everyday-conversations")

training_args = SFTConfig(
    max_seq_length=512,
    output_dir="/tmp",
)

trainer = SFTTrainer(
    model_name="HuggingFaceTB/SmolLM2-135M",
    train_dataset=dataset,
    args=training_args,
)
trainer.train()
```

<Tip>

✏️ **Try it out!** Use the `HuggingFaceTB/smoltalk` dataset to fine-tune a `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model.

For this exercise, you'll need to:
1. Load and prepare your chosen dataset
2. Configure the SFTTrainer with appropriate parameters
3. Train the model and monitor its progress
4. Save and evaluate the fine-tuned model

</Tip>

## Resources

- [TRL Documentation](https://huggingface.co/docs/trl)
- [SFT Examples Repository](https://github.com/huggingface/trl/tree/main/examples/sft)