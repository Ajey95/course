# Implementation with Transformers

Now that we understand how chat templates work, let's see how we can implement them using the `transformers` library. The transformers library provides built-in support for chat templates, we just need to use the `apply_chat_template()` method to format our messages.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-135M-Instruct")

messages = [
    {"role": "system", "content": "You are a helpful coding assistant."},
    {"role": "user", "content": "Write a Python function to sort a list"},
]

# Apply the chat template
formatted_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
```

This will return a formatted string that can be passed to the model. It would look like this for the SmolLM2-135M-Instruct model specified:

```sh
<|im_start|>system
You are a helpful coding assistant.<|im_end|>
<|im_start|>user
Write a Python function to sort a list<|im_end|>
```

Note that the `im_start` and `im_end` tokens are used to indicate the start and end of a message. The tokenizer will also have corresponding special tokens for the start and end of messages. For a refresher on how these tokens work, see the [Tokenizers](../chapter2/5.mdx) section.

Chat templates can handle multi-turn conversations while maintaining context:

```python
messages = [
    {"role": "system", "content": "You are a math tutor."},
    {"role": "user", "content": "What is calculus?"},
    {"role": "assistant", "content": "Calculus is a branch of mathematics..."},
    {"role": "user", "content": "Can you give me an example?"},
]
```

## Working with Chat Templates

When working with chat templates, you have several options for processing the conversation:

1. Apply the template without tokenization to return the raw formatted string
2. Apply the template with tokenization to return the token IDs
3. Add a generation prompt to prepare for model inference

The tokenizer's `apply_chat_template()` method handles all these cases through its parameters:

- `tokenize`: Whether to return token IDs (True) or the formatted string (False)
- `add_generation_prompt`: Whether to add a prompt for the model to generate a response

<Tip>

✏️ **Try it out!** Take a dataset from the Hugging Face hub and process it for Supervised Fine-Tuning (SFT). Convert the `HuggingFaceTB/smoltalk` dataset into chatml format and save it to a new file.

For this exercise, you'll need to:
1. Load the dataset using the Hugging Face datasets library
2. Create a processing function that converts the samples into the correct chat format
3. Apply the chat template using the tokenizer's methods

</Tip>

## Conclusion

Chat templates are a crucial component for working with language models, especially when fine-tuning or deploying models for chat applications. They provide structure and consistency to conversations, making it easier for models to understand context and generate appropriate responses.

Understanding how to work with chat templates is essential for:
- Converting datasets for fine-tuning
- Preparing inputs for model inference
- Maintaining conversation context
- Ensuring consistent model behavior

## Resources

- [Hugging Face Chat Templating Guide](https://huggingface.co/docs/transformers/main/en/chat_templating)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Chat Templates Examples Repository](https://github.com/chujiezheng/chat_templates) 
